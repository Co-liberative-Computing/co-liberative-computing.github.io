---
title: Data Feminism - Summary
call_to_action:
background_image_path:
large_header: false
show_in_navigation: false
navigation_order: 2
---

<div class="editable">
<br>
<p><img src="/uploads/book_data_feminism.jpg" style="width:30%; display: block; margin-left: auto; margin-right: auto;"></p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
Data Feminism, authored by Catherine D’Ignazio and Lauren F. Klein, explores how power and privilege shape the field of data science and offers a framework for rethinking data practices through an intersectional feminist lens. The book challenges traditional notions of objectivity in data, emphasizing the need to consider social, cultural, and historical contexts in how data is collected, analyzed, and represented. Organized into chapters that address topics like power, labor, and classification, the authors draw from diverse disciplines and real-world examples to highlight issues of inequality and propose actionable steps for creating more equitable and ethical data practices.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Introduction: Why Data Science Needs Feminism</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
The book's introduction explores the intersection of data science and feminist thought, emphasizing the importance of examining and challenging power structures in data practices. Beginning with the story of Christine Darden, a Black woman who broke barriers at NASA, the authors illustrate how systemic oppression in workplaces and society at large is often reflected in and reinforced by data systems. The book advocates for an intersectional feminist approach to data science, addressing the unequal distribution of power and proposing principles such as examining context, embracing pluralism, and making labor visible. Data Feminism seeks to inspire readers to use data as a tool for justice, challenging existing inequalities and reimagining a more equitable world.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">The Power Chapter</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This chapter examines how power influences data practices, revealing systemic bias and inequality. Serena Williams' childbirth experience highlights racial disparities in healthcare, with Black women in the US over three times more likely to die from pregnancy-related causes. The chapter introduces Patricia Hill Collins' "matrix of domination", describing how power operates through structural (laws), disciplinary (bureaucracy), hegemonic (culture), and interpersonal (individual) domains.
<br>
Data often reflects these inequalities, excluding marginalized groups or encoding bias, as seen in facial recognition failures and predictive algorithms targeting low-income families. These issues stem from a "privilege hazard", where dominant groups create systems blind to systemic oppression.
<br>
Change is possible through advocacy and innovation, like Joy Buolamwini's Algorithmic Justice League and Kimberly Seals Allers' Irth app for reviewing maternal care. The chapter calls for questioning who benefits, who is harmed, and whose priorities shape data science, aiming to dismantle inequities and create fairer systems.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Collect, Analyze, Imagine, Teach</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This chapter focuses on using data to reveal and counteract systemic inequalities. It discusses how power dynamics shape data collection and interpretation, often reflecting the biases of dominant groups. Two contrasting examples highlight this: a 1971 map by the Detroit Geographic Expedition and Institute exposing racial injustices in Black children's deaths caused by white commuters and a 1939 redlining map designed to maintain racial and economic segregation.
<br>
The authors emphasize how algorithms and data systems, like risk assessment tools, can perpetuate historical biases under the guise of neutrality. They argue that true equity requires addressing the root causes of oppression, not just technical flaws or individual biases. Strategies for challenging power include collecting "counterdata" to highlight systemic injustices, analyzing biases in existing systems, and imagining alternative, equitable futures.
<br>
The chapter also calls for a shift from "data ethics", which focuses on individual fairness and accountability, to "data justice", which centers on dismantling structural oppression and fostering collective liberation. Teaching data science with an emphasis on social justice, collaboration, and community engagement is presented as a way to empower marginalized groups and ensure data systems work toward equity.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This chapter highlights the importance of incorporating emotion and embodiment into data visualization to create more meaningful and inclusive representations. Traditional visualizations often aim for objectivity and neutrality, but these approaches mask biases and exclude marginalized voices. Instead, feminist perspectives advocate for "situated knowledge", acknowledging the cultural and personal contexts of data creation.
<br>
Periscopic's visualization of US gun deaths exemplifies this approach by focusing on "stolen years", emphasizing loss and grief rather than abstract statistics. Similarly, the chapter critiques the "god trick"—the illusion of neutral, all-seeing objectivity—arguing for transparency and the inclusion of diverse perspectives. The concept of "visceralization" is also introduced, using multisensory experiences like sound and touch to make data more accessible and impactful.
<br>
By rejecting the false divide between reason and emotion, the authors show how data communication can become more inclusive and effective, honoring the complexity of human experience while addressing systemic biases.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">What Gets Counted Counts</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This chapter discusses how data collection and classification systems, like those that categorize gender and race, often reinforce power imbalances and marginalize certain groups. For example, nonbinary people are often forced to choose between "male" or "female" in forms, which can feel exclusionary and harmful. Gender, like race, is a social construct, but current data systems typically use binary categories that don't reflect the complexity of human identity.
<br>
The chapter highlights how these classification systems can perpetuate inequality, such as through airport security screenings or Facebook's gender options. Feminist scholars argue for rethinking these systems to make them more inclusive and to challenge harmful binaries. The example of the Colored Conventions Project shows how collecting data thoughtfully can be empowering and help recover overlooked histories, like the contributions of Black activists.
<br>
It also discusses the ethical challenges of counting and classifying people, especially marginalized groups. Data collection should be done with consent and sensitivity to privacy risks, as it can both empower and expose people to harm. In conclusion, the chapter calls for a more thoughtful, intersectional approach to data collection, where counting can be a tool for accountability and social change rather than oppression.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Unicorns, Janitors, Ninjas, Wizards, and Rock Stars</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
In 2017, Bloomberg highlighted rising income inequality in the US, focusing on San Francisco, where wealth gaps intersect with race and gender disparities. Efforts like the Anti-Eviction Mapping Project (AEMP) document evictions and their impact on marginalized communities using maps and stories. Unlike traditional data projects, AEMP values collaboration with community organizations and employs feminist, antiracist approaches to resist displacement.
<br>
AEMP's work shows the power of pluralism—incorporating diverse voices at every stage of data projects. Examples like the Eviction Defense Collaborative partnership and creative outputs, such as maps and oral histories, demonstrate the importance of reflecting lived experiences in data work. This participatory approach builds solidarity and strengthens community voices against systemic inequalities.
<br>
By embracing pluralism, data projects can move beyond conventional "data for good" models toward co-liberation—empowering communities to own, interpret, and act on their data. Global initiatives like the Environmental Justice Atlas further illustrate how pluralistic, participatory data practices can scale while centering justice and local knowledge.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">The Numbers Don’t Speak for Themselves</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
In 2014, Boko Haram kidnapped 276 schoolgirls in Chibok, Nigeria, sparking global media attention. FiveThirtyEight published a data-driven article claiming over 3,600 kidnappings of young women in 2013, but their analysis relied on flawed data from GDELT, a project that misinterpreted repeated media reports as separate incidents. The error highlighted the risks of using big data without understanding its limitations.
<br>
This case underscores the importance of context in data science. Datasets like GDELT often lack transparency about their biases and collection methods, leading to misinterpretation. Feminist scholars stress that all data is "situated", shaped by power dynamics and collection environments, and warn against presenting data as neutral or self-explanatory. Misuse of context-free data can obscure systemic issues, such as racism or gender inequality, and perpetuate harm.
<br>
Efforts like "data biographies" and tools like SAFElab and Cuidando do Meu Bairro show how adding context can improve data use and empower marginalized communities. The Chibok case demonstrates that data must be understood in its social and historical context to ensure ethical and accurate analysis. Context isn't optional—it's essential for justice in data science.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Show Your Work</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
GitHub, widely used by software developers, has made collaboration easier but has also faced issues with inclusivity and corporate culture. Studies show women on GitHub are less likely to have their contributions accepted, and the company has faced allegations of discrimination and harassment. Despite these issues, GitHub's platform showcases the often invisible labor behind collaborative work, such as coding contributions, making this labor more visible.
<br>
Invisible labor extends beyond GitHub and is prevalent in data work. Projects like the Ship Map rely on extensive behind-the-scenes labor, from data collection to visualization. Often, this work is uncredited, especially in contexts where capitalism undervalues unpaid or underpaid contributions, such as crowdsourcing platforms like Amazon's Mechanical Turk. This mirrors historical patterns of exploitation, particularly of marginalized groups.
<br>
Feminist perspectives stress the importance of making this labor visible. Initiatives like the Atlas of Caregiving and tools developed for domestic workers aim to legitimize and support undervalued work, such as caregiving and emotional labor. By showing the hidden labor behind data and technology, we can challenge systemic inequalities and better recognize the human and environmental costs of modern data practices.
</p>

</div>
