---
title: Technocolonialism - Summary
call_to_action:
background_image_path:
large_header: false
show_in_navigation: false
navigation_order: 2
---

<div class="editable">
<br>
<p><img src="/uploads/book_techno_colonialism.jpg" style="width:30%; display: block; margin-left: auto; margin-right: auto;"></p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
Technocolonialism, by Mirca Madianou, examines how digital technologies are reshaping humanitarian aid in ways that often reproduce and reinforce longstanding colonial power structures. The book shows how tools promoted as solutions to humanitarian crises can, in fact, deepen inequality, erode individual agency, and subject vulnerable communities to new forms of surveillance and control. Through the concept of technocolonialism, Madianou highlights how these technologies, largely designed and governed in the Global North, extract data, impose external systems of classification, and reduce participation to mere symbolism, ultimately serving institutional interests rather than the needs of the people they are meant to support.
<br>
The book unfolds across six chapters, each examining a different layer of the digital humanitarian system: (1) the underlying motivations for adopting technology; (2) the racial and colonial roots of biometric systems; (3) the false promise of feedback mechanisms; (4) the use of refugee populations as experimental sites for untested technologies; (5) the emergence of the "humanitarian machine", a system that automates decision-making and distances humanitarians from those they aim to support; and (6) the everyday acts of resistance through which affected people assert their agency, adapt systems, or refuse them altogether. In highlighting both structural critique and situated resistance, Technocolonialism invites us to reimagine more just and accountable futures for technology and humanitarian aid.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">The Logics of Digital Humanitarianism</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
The first chapter of Technocolonialism explains why digital tools like biometric IDs, chatbots, and AI are becoming common in humanitarian work. This shift is part of what's called "digital humanitarianism", the growing use of technology during crises. However, as the chapter shows, this is not just about efficiency. It's tied to deep histories of colonialism, capitalism, and bureaucracy. Many different actors, from governments and aid groups to tech companies and donors, are involved, each with their own goals. This makes the humanitarian space complex and often contradictory, where digital tools can help but also harm.
<br>
To make sense of this complexity, the chapter outlines six key "logics" driving the use of digital technologies in aid: accountability, audit, capitalism, solutionism, securitization, and resistance. These logics show how digital systems are shaped by power: some are meant to track and measure aid, others to reduce costs or increase control. But people are not just passive recipients; many push back, adapt, or resist these systems. The idea of "technological solutionism" is especially criticized, as it is the belief that technology alone can fix deep social and political problems, which often leads to shallow or harmful interventions.
<br>
By tracing these six logics, the chapter shows that humanitarian technology is never neutral. It often reinforces the same inequalities it claims to address, especially when it reflects the interests of powerful governments, donors, or corporations. Even well-meaning tools can turn aid into surveillance or participation into a checkbox. However, the chapter also emphasizes that people affected by these systems still find ways to assert their voice and challenge the rules. These logics set the stage for the rest of the book, which digs deeper into the impacts of digital tools on real lives and communities.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Biometric Infrastructures</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This chapter shows how biometric systems, like iris scans and fingerprints, are now widely used in refugee camps and humanitarian aid, promoted as tools to improve efficiency and prevent fraud. Programs like the World Food Programme's Building Blocks combine biometrics with blockchain to manage identities and distribute aid, but this often limits people's choices. Instead of empowering refugees, these systems create strict controls over how they shop, where they move, and even what kind of aid they receive. Though they appear modern, these technologies continue patterns of colonial control, using people's bodies as sites of surveillance and reducing freedom in the name of innovation.
<br>
Biometrics are not neutral. They are rooted in racist and colonial histories and often reflect bias in design and implementation. Systems trained mostly on white faces often perform poorly for women and people of color. In refugee settings, a small error can mean losing access to food or safety. Worse, sensitive data is often shared with governments and private companies without clear consent or oversight, leading to serious risks when data falls into the wrong hands. The systems are presented as secure and transparent, but many refugees are not properly informed or given a real choice about participation, making "consent" more symbolic than real.
<br>
The chapter argues that what's framed as digital progress often hides deeper injustices. Technologies like "digital wallets" limit rather than expand agency, and promises of empowerment often mask top-down systems of control. Aid that depends on biometrics can recreate the very hierarchies it claims to dismantle, turning humanitarian help into a mechanism for managing and disciplining vulnerable populations. The term technocolonialism captures this dynamic: a continuation of colonial power through digital systems that track, restrict, and classify people while offering little room for voice, resistance, or real autonomy.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Extracting Data and the Illusion of Accountability</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
After Typhoon Haiyan, aid agencies in the Philippines used digital tools like SMS hotlines to collect feedback, aiming to show they were "accountable" to the people they served. However, this accountability was narrow; it only applied to certain projects and often ignored those left out of aid. People like Dolores, who asked for help but didn't qualify, were met with silence or automated replies. Instead of improving aid, these systems mostly collected data for reports, helping agencies secure funding rather than address real needs. Many people felt ignored, and digital feedback became more about appearances than listening.
<br>
Digital tools like chatbots, apps, and online forms were seen as efficient, but they excluded many vulnerable people, like those without phones, literacy, or internet access. These systems also shaped the kind of feedback allowed, forcing people to use preset categories or limited options. While marketed as tools for inclusion, they often made people feel more distant from aid organizations. Instead of closing the feedback loop, these tools left many without answers, reinforcing feelings of frustration, exclusion, and powerlessness.
<br>
The feedback collected from affected communities usually traveled up to headquarters and donors, not back to the people who gave it. Aid organizations used this data to justify their work and attract funding, while those in need saw little change. This one-way flow of information, where data moves from the Global South to powerful institutions in the Global North, is what the book calls technocolonialism. It exposes how digital tools, though framed as participatory, often reproduce old patterns of inequality, extracting value from people's experiences while giving them little in return.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Surreptitious Experimentation: Enchantment, Coloniality and Control</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
In crisis-affected communities, humanitarian organizations and tech companies often test unproven technologies without proper consent or oversight. For example, a chatbot designed to help Syrian refugees with mental health was built in Silicon Valley and tested on a small group without therapists or safeguards. These trials are framed as innovation or "AI for good", but they often repeat colonial patterns: imposing Western-designed tools on people in the Global South, collecting data, and offering little in return. Refugees may not even realize they are part of an experiment, especially when these tools are built into everyday services like shopping or messaging.
<br>
This hidden testing, what the author calls surreptitious experimentation, uses people's data, time, and behavior for outside gain. These technologies are often built in hackathons far from their target communities and shaped by foreign values. While marketed as empowering or efficient, they mostly benefit aid organizations, private firms, and donors through publicity, data, and credibility. Meanwhile, affected people are left with limited choices, often forced to use unfamiliar or incomplete systems just to access basic aid. These tools blur the lines between care and control, erasing local voices and reinforcing unequal power.
<br>
The illusion of progress is kept alive by the "enchantment" of technology, its ability to seem neutral, advanced, or magical. Humanitarian chatbots and digital wallets may look innovative, but they hide the labor, bias, and risks behind them. When tools like these are promoted without transparency or safeguards, they can cause real harm, especially in already fragile settings. This chapter shows how today's digital experiments are not just about testing technology but about who gets to decide, who is at risk, and who profits, revealing how old colonial dynamics continue through modern humanitarian tech.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">The Humanitarian Machine: Automating Harm</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
Humanitarian organizations increasingly rely on AI and digital tools to appear modern, efficient, and objective. These systems, however, often ignore the complexities of real human lives. Decisions made by algorithms can be unfair, difficult to challenge, and deeply harmful. People are treated as numbers in rigid systems that don't allow for exceptions or mistakes. Tools like biometric databases, feedback apps, or hunger maps may seem helpful, but they often hide the human labor, bias, and politics behind them. The system gives the appearance of control and progress while sidelining care, empathy, and real listening.
<br>
This system, what the author calls the "humanitarian machine", runs on data, bureaucracy, and partnerships with tech companies. It's not just a metaphor but a global structure connecting aid agencies, governments, and private firms. These actors all have different goals, from funding and security to profit and innovation. Meanwhile, affected communities often remain voiceless. Even well-meaning aid workers are trapped in systems prioritizing reporting, audits, and digital tools over local knowledge and lived experience. Automation deepens these issues by removing human judgment and creating even more distance between decision-makers and the people affected.
<br>
Despite growing awareness of these problems, real change is rare. The humanitarian system tends to respond to criticism with more layers of bureaucracy rather than shifting power or accountability. Local actors remain marginalized, while big organizations and tech firms continue to dominate. Digital tools promise fairness and efficiency but often repeat old colonial patterns, extracting data, enforcing control, and treating people as objects of management. The humanitarian machine keeps running, absorbing critique but doing little to center the dignity, voices, or needs of those it claims to serve.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Mundane Resistance: Contesting Technocolonialism in Everyday Life</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
After Typhoon Haiyan, people in the Philippines used digital tools in unexpected ways. While aid agencies set up radio shows and SMS platforms to collect feedback, many locals sent in song requests and messages to loved ones instead. These acts helped them feel connected and normal again. Though small, they quietly pushed back against top-down systems that only saw technology as a tool for data collection. By using platforms in ways that mattered to them, whether to sing, joke, or remember the dead, people reclaimed some control in a system that often ignored their voices.
<br>
The chapter calls these everyday actions "mundane resistance". This includes ignoring feedback tools, using them differently, or refusing to share personal data. Even silence can be a form of protest when systems feel untrustworthy or imposed. People also used social media to mourn, tell their stories, and organize small campaigns, like pressuring local officials to deliver aid. These creative acts don't always lead to big changes, but they challenge power quietly and meaningfully. Whether it's a refugee sharing music online or someone refusing to use a chatbot, these moments reveal human agency in the face of technocolonial systems.
<br>
Still, resistance isn't equally possible for everyone. In refugee camps, for example, people often have no choice but to give up biometric data. But even in such restricted settings, some find ways to push back through private messages, local humor, or choosing not to engage. The chapter argues that both structural injustice and everyday defiance matter. While mundane resistance may not dismantle systems on its own, it keeps alive a sense of dignity and possibility. These small acts remind us that people don't simply accept control; they adapt, reshape, and sometimes quietly refuse it.
</p>
<br>
<p align="justify" style="font-family:'Arial';font-size:28px;font-weight: bold;">Conclusion: Technocolonialism as Infrastructural Violence</p>
<p align="justify" style="font-family:'Arial';font-size:20px;">
This book shows how digital tools used in humanitarian aid often repeat old patterns of control and inequality, especially in the Global South. The term technocolonialism is used to describe how technologies like biometric IDs, chatbots, and feedback systems, usually designed in the West, collect data from vulnerable people, often without real consent. While these tools are supposed to help, they often end up serving governments, aid agencies, or tech companies more than the communities they target. Instead of improving aid, they sometimes block access or harm those in need, especially when systems are biased or fail.
<br>
The book explains how these digital systems are part of a larger structure, or infrastructure, that connects governments, corporations, and NGOs. This network blurs lines of responsibility and makes it hard to know who is accountable when harm is done. Biometric systems, algorithmic decisions, and automated platforms can deny aid or control people's movements while hiding behind the idea of neutrality. These technologies often assume mistrust, especially of refugees or poor communities, and treat people as data points, not as individuals. This form of quiet, indirect harm is called infrastructural violence.
<br>
Despite the harm, people find ways to push back by ignoring platforms, using tech for their own purposes, or refusing to give data. The book calls these everyday actions mundane resistance. However, resisting technocolonialism requires more than small acts; it requires collective organizing and structural change. The author urges us to rethink aid, question the power behind tech, and imagine new forms of solidarity rooted in justice. Rather than fixing broken tools, we need to build different systems, ones that truly serve the people they are meant to help.

</p>
</div>
